{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a21bdaa-9df8-4503-bfc0-39e2d7efa766",
   "metadata": {},
   "source": [
    "# MLOPs Parsl workflow\n",
    "\n",
    "This notebook is the stand-alone companion to the Parsl MLOPs workflow in `main.py` in this repository. This notebook is designed to be run directly on an HPC resource while the `main.py` in this workflow uses the `parsl_utils` to launch MLOPs applications from a central coordinating node (i.e. a laptop or the Parallel Works platform). This workflow simulates a typical MLOPs situation with the following tasks:\n",
    "1. start an MLFlow tracking server\n",
    "2. start DVC tracking within an architve repository + remote\n",
    "3. download and preprocess training data\n",
    "4. run training loop and store results on-the-fly with MLFlow\n",
    "5. commit and push resulting models with DVC to repo + remote\n",
    "6. use the model for inference and generate figures.\n",
    "7. reusing the model for inference and generating figures\n",
    "\n",
    "The core ML training used here started as a copy of [Francois Chollet's VAE digits example](https://keras.io/examples/generative/vae/) but modified for online learning, MLOPs, and Parsl orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768cea9-9406-4d21-8d60-2d78ff2b9182",
   "metadata": {},
   "source": [
    "## Installs\n",
    "\n",
    "The bulk of the install commands below are commented out \n",
    "since it is faster to reconstruct a Conda environment from\n",
    "an exported env file (.yaml) than to rebuild from scratch.\n",
    "That reconstruction command is kept active here since \n",
    "env files are distributed with this notebook. Note that there\n",
    "are two different environments - one for CPU and one for GPU -\n",
    "TensorFlow will not in general work if you use . Once the command to reconstruct\n",
    "the Conda environment has been run, you may need to tell\n",
    "this notebook to use the kernel from that Conda environment\n",
    "with the `Kernel > Change kernel...` option in the menu above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34799ce1-6bf0-4862-a4ae-f0f089701bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_from_scratch=False\n",
    "\n",
    "if (install_from_scratch):\n",
    "    # Use a specific version of Python due to\n",
    "    # compatibility with TensorFlow and Keras\n",
    "    #! conda create --name mlops-parsl-gpu python=3.9\n",
    "    \n",
    "    # To use a Jupyter notebook with a\n",
    "    # specific conda environment:\n",
    "    ! conda install -y requests\n",
    "    ! conda install -y ipykernel\n",
    "    ! conda install -y -c anaconda jinja2\n",
    "    \n",
    "    # Conda installs\n",
    "    ! conda install -y -c conda-forge matplotlib\n",
    "    ! conda install -y -c conda-forge pandas\n",
    "    ! conda install -y -c conda-forge dvc \n",
    "    \n",
    "    # pip installs\n",
    "    ! pip install --upgrade pip\n",
    "    ! pip install tensorflow # For CPU nodes, will work on GPU nodes but will be slow\n",
    "    #! pip install tensorflow[and-cuda] # Replace the above with this line for GPU nodes\n",
    "    ! pip install tensorflow-plugin-profile\n",
    "    ! pip install mlflow\n",
    "    ! pip install 'parsl[monitoring, visualization]' # Conda does not install monitoring, so use pip\n",
    "\n",
    "    # The environment was then exported with:\n",
    "    # ! conda env export --name mlops-parsl-cpu > ./requirements/mlops-parsl-cpu.yaml\n",
    "else:\n",
    "    # You can rebuild the environment with:\n",
    "    ! conda env update -f ./requirements/mlops-parsl-cpu.yaml --name mlops-parsl-cpu\n",
    "\n",
    "# Note that there are two exported Conda environments\n",
    "# - *cpu and *gpu - because the installation of TensorFlow \n",
    "# will automatically detect the presence of GPUs and \n",
    "# change which version is installed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b018e6-3147-4123-ab59-40146b498675",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Based on the instructions in the [Parsl Tutorial](https://parsl.readthedocs.io/en/latest/1-parsl-introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576f915-2745-4aef-843e-4aa1986d6130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# ml dependencies\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "# mlflow dependencies\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# parsl dependencies\n",
    "import parsl\n",
    "import logging\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import Config\n",
    "from parsl.executors import HighThroughputExecutor # we want to use monitoring, so we must use HTEX\n",
    "from parsl.monitoring.monitoring import MonitoringHub\n",
    "from parsl.addresses import address_by_hostname\n",
    "\n",
    "#=================================================\n",
    "# Log everything to stdout (ends up in pink boxes \n",
    "# in the notebook). This information is logged anyway\n",
    "# in ./runinfo/<run_id>/parsl.log\n",
    "#parsl.set_stream_logger() # <-- log everything to stdout\n",
    "#==================================================\n",
    "\n",
    "print(parsl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6be29-0198-4394-9c9c-94b2f3e10635",
   "metadata": {},
   "source": [
    "# Configure Parsl\n",
    "\n",
    "This configuration must use the HTEX since we also want to enable [Parsl monitoring](https://parsl.readthedocs.io/en/latest/userguide/monitoring.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57095f-292c-4b32-ae49-8382ec2fdede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "   executors=[\n",
    "       HighThroughputExecutor(\n",
    "           label=\"local_htex\",\n",
    "           cores_per_worker=0.1,\n",
    "           max_workers_per_node=4,\n",
    "           address=address_by_hostname(),\n",
    "       )\n",
    "   ],\n",
    "   monitoring=MonitoringHub(\n",
    "       hub_address=address_by_hostname(),\n",
    "       hub_port=55055,\n",
    "       monitoring_debug=False,\n",
    "       resource_monitoring_interval=10,\n",
    "   ),\n",
    "   strategy='none'\n",
    ")\n",
    "\n",
    "# Loading the configuration starts a Parsl DataFlowKernel\n",
    "dfk = parsl.load(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c0f08-efee-4c9e-8191-f6373e7238e4",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 1 - direct shell invocation to background\n",
    "\n",
    "This step can be done at any point provided that a database file exists.  The default location of this file is in `./runinfo/monitoring.db` and this file is created when the Parsl configuration is loaded. When the notebook kernel is restarted, additional Parsl workflow runs' information is appended to the monitoring information in `./runinfo`. It is possible to view this information \"offline\" (i.e. no active running Parsl workflows, see Option 3, at the end of this notebook).\n",
    "\n",
    "This launch is commented out here since it is also possible to launch `parsl-visualize` from a Parsl app within the workflow, which is done below. This command is retained as a functional example. The advantage to running `parsl-visualize` as a Parsl app is that the visualization server is up and running while the workflow is running and then is shut down when the workflow is cleaned up. Otherwise, when `parsl-visualize` is launched via `os.system` the running child process can persist even after workflow shut down or notebook kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444b703-daee-4ffa-b111-7c7dcd277c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch Parsl \n",
    "#os.system('parsl-visualize 1> parsl_vis.stdout 2> parsl_vis.stderr &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471719b-de46-473f-8444-295ad8e51914",
   "metadata": {},
   "source": [
    "## Define Parsl apps\n",
    "\n",
    "Parsl workflows are divided into the smallest unit of execution, the app. There are two types of Parsl apps:\n",
    "1. Python apps are useful when launching pure Python code (i.e. TensorFlow)\n",
    "2. Bash apps are useful when launching tasks on the command line (i.e. starting the MLFlow server)\n",
    "\n",
    "Here, the applications are *defined* but not run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddeb06-9d92-4290-bb7f-048b1f506ddb",
   "metadata": {},
   "source": [
    "### Python Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c288da-dbd2-49a4-aaad-8a67c472d171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def make_dir(model_dir):\n",
    "    import os\n",
    "    os.makedirs(model_dir, exist_ok = False)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a816c-b89a-440e-9e8e-7fd9acae916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def build_train_model(inputs=[], outputs=[]): # inputs = [data, experiment, num, build]\n",
    "    \n",
    "    # imports ---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        f.write(\"Starting training app...\\n\")\n",
    "        f.write(\"Execution is in \"+os.getcwd()+\"\\n\")\n",
    "    \n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "    # ml dependencies\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "\n",
    "    # mlflow dependencies\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    \n",
    "    # definition library\n",
    "    import sys\n",
    "    sys.path.append(os.getcwd())\n",
    "    from definitions import Sampling, VAE\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        f.write(\"Building model...\\n\")\n",
    "    \n",
    "    # build encoder ---------------------------------------------------------------------------------------\n",
    "    \n",
    "    latent_dim = 2\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        encoder.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # build decoder ---------------------------------------------------------------------------------------\n",
    "    \n",
    "    latent_dim = 2\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        decoder.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # build model -----------------------------------------------------------------------------------------    \n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "    \n",
    "    # train model ----------------------------------------------------------------------------------------- \n",
    "    \n",
    "    \n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        f.write(\"Set up training context...\\n\")\n",
    "    \n",
    "    model_dir = './model_dir' \n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\n",
    "    \n",
    "    # if the model has already been trained at least once, load that model\n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): \n",
    "        with open('model_build_out.txt', 'a') as f:\n",
    "            f.write(\"found weights\\n\")\n",
    "        vae.load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "    \n",
    "    mlflow.autolog() # start autologging\n",
    "    \n",
    "    run_name = f\"{inputs[2]}_test\" # define a run name for this iteration of training\n",
    "    artifact_path = f\"{inputs[2]}\"  # define an artifact path that the model will be saved to\n",
    "    \n",
    "    # initiate the MLflow run context \n",
    "    # - training needs to happen inside of the mlflow run or you will run into problems with double logging\n",
    "    with mlflow.start_run(run_name = run_name, experiment_id = inputs[1]) as run:\n",
    "        \n",
    "        with open('model_build_out.txt', 'a') as f:\n",
    "            f.write(\"Core model training...\\n\")\n",
    "        history = vae.fit(inputs[0], epochs=30, batch_size=128, callbacks = [early_stopping_cb])\n",
    "        \n",
    "        with open('model_build_out.txt', 'a') as f:\n",
    "            f.write(\"Save model weights...\\n\")\n",
    "        vae.save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "\n",
    "        with open('model_build_out.txt', 'a') as f:\n",
    "            f.write(\"Getting and saving model training history...\\n\")\n",
    "        hist_pd = pd.DataFrame(history.history)\n",
    "        hist_pd.to_csv(os.path.join(model_dir, f'history_{inputs[2]}.csv'), index = False)\n",
    "    \n",
    "    with open('model_build_out.txt', 'a') as f:\n",
    "        f.write(\"Training app done.\\n\")\n",
    "        \n",
    "    return 1\n",
    "    # figure out how to clear model data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc570-c425-4eda-9c57-73191da7d946",
   "metadata": {},
   "source": [
    "### Bash Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109f82b-c83f-4fcc-8718-6254ed1b5ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def start_parsl_visualize(stdout='parsl_vis_app.stdout', stderr='parsl_vis_app.stderr'):\n",
    "    return 'parsl-visualize --listen 127.0.0.1 --port 8080'\n",
    "\n",
    "@bash_app\n",
    "def start_mlflow(stdout='mlflow_app.stdout', stderr='mlflow_app.stderr'):\n",
    "    return 'mlflow server --host 127.0.0.1 --port 8081'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df7db9-922a-4871-af78-2c475dfdc854",
   "metadata": {},
   "source": [
    "## Start MLFlow and start Parsl monitoring - Option 2 - Monitoring as a Parsl app\n",
    "\n",
    "This approach is helpful if we want Parsl Monitoring processes to be cleaned up after the workflow is complete. This point is also the most natural time to start MLFlow. Note that both of these commands are tracked by Parsl and are considered to be part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea30929-87b0-442d-a5aa-ee9cd7a7ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Parsl visualization in a\n",
    "# separate cell since we only want\n",
    "# to run this app one time. This\n",
    "# invocation of parsl_visualize is\n",
    "# technically part of the workflow.\n",
    "\n",
    "parsl_vis_future = start_parsl_visualize()\n",
    "mlflow_future = start_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f20000-a71e-4702-b224-590d719504fd",
   "metadata": {},
   "source": [
    "## Run the workflow\n",
    "\n",
    "The workflow code below runs the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eea978-6be7-4611-b7ff-a6afc11500b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model directory that holds information on the model\n",
    "model_dir = './model_dir' \n",
    "\n",
    "mkdir_future = make_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439ff77-9b88-44b3-801b-07e668d472b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utilize and set up the initialized server for tracking \n",
    "client = MlflowClient(tracking_uri = \"http://127.0.0.1:8081\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8081\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eebc11-895f-4b13-b0f3-19897c98d7f9",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "In Experiment 1, we train the Digit CVAE model on multiple datasets. To create these datasets, we split the original dataset into five equal, randomized parts. After each training session, we save the weights and use them as the starting point for retraining the model on the next dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869afd68-78ed-4c27-9008-bcc33aa174d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# provide an experiment description that will appear in the UI\n",
    "experiment1_description = (\n",
    "    \"This is the digits forecasting project.\"\n",
    "    \"This experiment contains the digit model for randomized numbers (0-9) trained separately.\"\n",
    ")\n",
    "\n",
    "# provide searchable tags for the experiment\n",
    "experiment1_tags = {\n",
    "    \"project_name\": \"digit-forecasting\",\n",
    "    \"model_type\": \"randomzied\",\n",
    "    \"team\": \"digit-ml\",\n",
    "    \"project_quarter\": \"Q3-2024\",\n",
    "    \"mlflow.note.content\": experiment1_description,\n",
    "}\n",
    "\n",
    "# create the experiment and give it a unique name\n",
    "digit_experiment1 = client.create_experiment(\n",
    "    name=\"Randomize_Model\", tags=experiment1_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61055cfa-9a8d-45dc-8d02-8dcf72c70fe0",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "In Experiment 2, we train the Digit CVAE model on all digit samples simultaneously, without any subsequent retraining using the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcda18-a9da-4150-b4af-29bd8dac90ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# provide an experiment description that will appear in the UI\n",
    "experiment2_description = (\n",
    "    \"This is the digits forecasting project.\"\n",
    "    \"This experiment contains the digit model for numbers (0-9) trained all together.\"\n",
    ")\n",
    "\n",
    "# provide searchable tags for the experiment\n",
    "experiment2_tags = {\n",
    "    \"project_name\": \"digit-forecasting\",\n",
    "    \"model_type\": \"all digits\",\n",
    "    \"team\": \"digit-ml\",\n",
    "    \"project_quarter\": \"Q3-2024\",\n",
    "    \"mlflow.note.content\": experiment2_description,\n",
    "}\n",
    "\n",
    "# create the experiment and give it a unique name\n",
    "digit_experiment2 = client.create_experiment(\n",
    "    name=\"Together_Model\", tags=experiment2_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc8487-e83a-42f2-a3da-26fe0f45bcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save each of the experiment's metadata\n",
    "digit_experiment1 = mlflow.set_experiment(\"Randomize_Model\")\n",
    "digit_experiment2 = mlflow.set_experiment(\"Together_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94da8b-8b03-483a-8eb3-4f3d48b18edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retraining the model n times\n",
    "(x_train, Y_train), (x_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.expand_dims(np.concatenate([x_train, x_test], axis=0), -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcff868-b201-4ce1-9599-ec385876aec5",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf945321-9458-46ee-98c4-3715a0cdaaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retraining the model n times\n",
    "count = 0\n",
    "build = []\n",
    "\n",
    "for arr in np.array_split(mnist_digits, 5):\n",
    "    count += 1\n",
    "    \n",
    "    if (count > 1):\n",
    "        print('Launching retraining...')\n",
    "        # Note that we augment the counter above\n",
    "        # for the next training iteration BUT the\n",
    "        # .append() operation only happens after the\n",
    "        # execution of the code inside the (). This\n",
    "        # means that we need to reference build[count-2]\n",
    "        # (and not -1) because we haven't appended\n",
    "        # the future to the future list until the\n",
    "        # app is launched, so we need to use the counter\n",
    "        # the corresponds to the future list before\n",
    "        # the launch happens.\n",
    "        enforce = build[count-2]\n",
    "    else:\n",
    "        print('Launching first training...')\n",
    "        enforce = 0\n",
    "    \n",
    "    # Launch training\n",
    "    build.append(\n",
    "        build_train_model(\n",
    "            inputs=[\n",
    "                arr, \n",
    "                digit_experiment1.experiment_id, \n",
    "                f\"rand_{count}\", enforce]))\n",
    "    \n",
    "    # Print the future status of the launched app\n",
    "    print(build[count-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ef487-3384-4fc3-8511-854d15e1a24f",
   "metadata": {},
   "source": [
    "#### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44905b89-c005-4e2f-90d5-b0c616533b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train all numbers at the same time\n",
    "build = build_train_model(inputs=[mnist_digits, digit_experiment2.experiment_id, \"all\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c37b05-e95d-4ff4-9167-065e69dac6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce8a6d-d587-4ea1-b783-c8a7e56a0112",
   "metadata": {},
   "source": [
    "#### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d02aea-e2a3-49db-88fe-9fca195b7955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063ba3c8-6a15-48b7-a054-633d981529c4",
   "metadata": {},
   "source": [
    "## DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677e1e7-2d9d-4a1b-a9f9-384b75650185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app # grab your dvc repository\n",
    "def add_submodule(stdout='dvc.stdout', stderr='dvc.stderr'):\n",
    "    dvc_repo_link=\"git@github.com:oobielodan/digits_dvc.git\" # <ssh link to the repo you set aside for dvc>\n",
    "    \n",
    "    return 'git submodule add --force $dvc_repo_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d7f63-5a53-4f86-adf2-0d519e67f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app # DVC initialization and storage set up\n",
    "def dvc_setup(stdout='dvc.stdout', stderr='dvc.stderr'):\n",
    "    dvc_storage=\"/demo-bucket\" # <complete path to the mounted storage you have set up for dvc>\n",
    "    dvc_repo=\"digits_dvc\"\n",
    "    \n",
    "    return 'cd $dvc_repo && dvc init && dvc remote add -d dvcstorage $dvc_storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc813783-e06e-48f0-bdce-9fa6861eab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app # initial commit to git\n",
    "def dvc_init(stdout='dvc.stdout', stderr='dvc.stderr'):\n",
    "    dvc_storage=\"/demo-bucket\" # <complete path to the mounted storage you have set up for dvc>\n",
    "    dvc_repo=\"digits_dvc\"\n",
    "    \n",
    "    return 'cd $dvc_repo && git add . && git commit -m \"loaded dependencies, mkdir -p, DVC init\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c58fce-00d6-406f-ae6b-3fbee5db1b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def execute_dvc(stdout='dvc.stdout', stderr='dvc.stderr'):    \n",
    "    dvc_repo=\"digits_dvc\"\n",
    "    env_name=\"mlops-parsl-cpu\" # <name of your env>\n",
    "    model_dir=\"model_dir\"\n",
    "    \n",
    "    return 'cp ./$model_dir/vae.weights.h5 $dvc_repo/experiment_2.weights.h5 && sh dvcgit.sh experiment_2.weights.h5 \"digit experiment 2\" $dvc_repo $env_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546984e-f2ca-4734-9b7e-4d3bf337c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def rm_dvc(stdout='dvc.stdout', stderr='dvc.stderr'):\n",
    "    dvc_repo=\"digits_dvc\"\n",
    "    model_dir=\"model_dir\"\n",
    "    \n",
    "    return 'rm $dvc_repo/experiment_2.weights.h5 && rm ./$model_dir/vae.weights.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676b6bb-3973-4bec-8a6c-b2933fa9ddfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_add = add_submodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e483a4a-89bb-4b94-acb5-b9b769f5bb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee1f08-220b-43eb-af7e-e93d102fd82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_setup = dvc_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64715122-d933-4978-aafe-b86bd70d3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5dbf2-12df-48fc-b97b-be30fca2cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_init = dvc_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8445c-23b3-4536-a1b0-be2d21397b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01d358-1321-47f8-978a-7c2ee8426384",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_execute = execute_dvc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb40bb-2005-4a7c-9b26-58993160be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7d7b7-35e1-4827-87cb-80053818b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "futurre_rm = rm_dvc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9316f77-9f87-4874-a6da-d012f6773537",
   "metadata": {},
   "outputs": [],
   "source": [
    "futurre_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742f3ea-2d2d-407f-948b-bdb583c15184",
   "metadata": {},
   "source": [
    "## Stop Parsl\n",
    "\n",
    "The cells above can be rerun any number of times; this will simply send more and more apps to be run by Parsl. When the workflow is truly complete, it is time to call the cleanup() command. This command runs implicitly when a `main.py` script finishes executing, but it is *not* run in a notebook unless it is explicitly called as it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2aee4-6d3d-4df8-b2e0-991c10db793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d3a0-3009-472e-b82d-3d4b5566dde1",
   "metadata": {},
   "source": [
    "## Clean up some log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3284cf0-f754-4d40-bf69-92d38cbe9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe write a script for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1af19-9196-4a41-b63d-411ebbc52bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Application logs\n",
    "! rm mlflow_app.stdout\n",
    "! rm mlflow_app.stderr\n",
    "! rm model_build_out.txt\n",
    "\n",
    "# Remove log files if parsl-visualize is started from os.system (Option 1)\n",
    "! rm parsl_vis.stdout\n",
    "! rm parsl_vis.stderr\n",
    "\n",
    "# Remove log files if parsl-visualize is started from Parsl app (Option 2)\n",
    "! rm parsl_vis_app.stdout\n",
    "! rm parsl_vis_app.stderr\n",
    "\n",
    "# This directory contains Parsl monitoring along with other logs\n",
    "! rm -rf runinfo\n",
    "\n",
    "# This directory contains the saved model files\n",
    "! rm -rf model_dir\n",
    "\n",
    "# This directory contains the databases for MLFlow\n",
    "! rm -rf mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3341c74-f45a-4851-a744-f12657c29c3b",
   "metadata": {},
   "source": [
    "## Start Parsl Monitoring - Option 3 - Post workflow manual invocation\n",
    "\n",
    "Once the Parsl `./runinfo/monitoring.db` is created, it is possible to start Parsl Monitoring and browse the results of workflow in an offline manner.  In this scenario, `parsl-visualize` can be started on the command line provided that a Conda env with `parsl[visualize]` installed is activated. For example:\n",
    "```\n",
    "source pw/.miniconda3/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "parsl-visualize sqlite:////${HOME}/mlops-parsl-workflow/runinfo/monitoring.db\n",
    "```\n",
    "(You may need to adjust the path to the Conda environment, its name, and the path to `monitoring.db`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486c478-d339-4993-9ae0-3750ea1dda49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlops-parsl-cpu]",
   "language": "python",
   "name": "conda-env-mlops-parsl-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
